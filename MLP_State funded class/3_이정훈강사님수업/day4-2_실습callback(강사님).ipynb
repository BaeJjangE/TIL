{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4021b4a",
   "metadata": {},
   "source": [
    "## 참고: https://hleecaster.com/ml-linear-regression-example/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d4f2b0",
   "metadata": {},
   "source": [
    "## 라이브러리 설치, 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2853ab4",
   "metadata": {},
   "source": [
    "!pip3 install -U scikit-learn<br>\n",
    "!pip3 install pandas<br>\n",
    "!pip3 install numpy<br>\n",
    "!pip3 install matplotlib<br>\n",
    "!pip3 install statsmodels<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c2d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a92498",
   "metadata": {},
   "source": [
    "## 데이터 다운로드 (특별할인 판매)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbdefba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "TRAIN = pd.read_csv('train.csv')\n",
    "VALID = pd.read_csv('valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82009ce6",
   "metadata": {},
   "source": [
    "# Input, Feature 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26711f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_y = TRAIN[['Relapse']]\n",
    "TRAIN_x = TRAIN[ TRAIN.columns[1:] ]\n",
    "\n",
    "VALID_y = VALID[['Relapse']]\n",
    "VALID_x = VALID[ VALID.columns[1:] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ec0b3",
   "metadata": {},
   "source": [
    "## Keras Logit 모델 fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca83f5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VALID_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fdfaee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(5, activation='linear', input_shape=(8,)))\n",
    "model.add(Dense(10, activation='linear'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# CallBack 함수를 통해 LR을 낮출 것이므로, 초기 LR을 높게 잡기\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "#0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "daccfab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 5)                 45        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                60        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 116\n",
      "Trainable params: 116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d903ad",
   "metadata": {},
   "source": [
    "## CALL BACK 함수 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c14e1c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call-back 함수\n",
    "# CheckPoint: Epoch 마다 validation 성능을 검증하여, best performance 일 경우 저장\n",
    "CP = ModelCheckpoint(filepath='{epoch:03d}-{loss:.4f}-{accuracy:.4f}-{val_loss:.4f}.hdf5',\n",
    "            monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "#minitor -> loss -> val_loss\n",
    "\n",
    "# Learning Rate 줄여나가기\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=4,\n",
    "                       verbose=1, min_lr=1e-8)\n",
    "# factor: Learning rate에 곱할 것.\n",
    "#0.1 -> 0.08 -> 0.064 ....\n",
    "#monitor='loss' -> monitor='val_loss'\n",
    "\n",
    "CALLBACK = [CP, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "832349d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75ac0716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 2.2302 - accuracy: 0.5440 - val_loss: 0.7795 - val_accuracy: 0.4898\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.62259\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1.1554 - accuracy: 0.5206 - val_loss: 1.0190 - val_accuracy: 0.3878\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.62259\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.8086 - accuracy: 0.5732 - val_loss: 0.6117 - val_accuracy: 0.7245\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62259 to 0.61167, saving model to 003-0.7639-0.6122-0.6117.hdf5\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6605 - accuracy: 0.6107 - val_loss: 0.6427 - val_accuracy: 0.6531\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.61167\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6390 - accuracy: 0.6595 - val_loss: 0.6426 - val_accuracy: 0.6429\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.61167\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6782 - accuracy: 0.6023 - val_loss: 0.6889 - val_accuracy: 0.6224\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61167\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.8419 - accuracy: 0.5781 - val_loss: 1.0896 - val_accuracy: 0.3878\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.005999999865889549.\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.8853 - accuracy: 0.5428 - val_loss: 0.7239 - val_accuracy: 0.5102\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61167\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.8290 - accuracy: 0.5790 - val_loss: 0.6610 - val_accuracy: 0.6224\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61167\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6645 - accuracy: 0.6441 - val_loss: 0.6165 - val_accuracy: 0.6837\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61167\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6338 - accuracy: 0.6326 - val_loss: 0.6411 - val_accuracy: 0.6837\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.003600000031292438.\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6356 - accuracy: 0.6340 - val_loss: 0.7197 - val_accuracy: 0.6224\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.61167\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6782 - accuracy: 0.6193 - val_loss: 0.6212 - val_accuracy: 0.6735\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.61167\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6311 - accuracy: 0.6321 - val_loss: 0.6640 - val_accuracy: 0.6122\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.61167\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6801 - accuracy: 0.6158 - val_loss: 0.6151 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0021599999628961085.\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6159 - accuracy: 0.6841 - val_loss: 0.6183 - val_accuracy: 0.6633\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.61167\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6457 - accuracy: 0.6436 - val_loss: 0.6383 - val_accuracy: 0.6429\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.61167\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6635 - accuracy: 0.6103 - val_loss: 0.6162 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.61167\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6396 - accuracy: 0.6478 - val_loss: 0.6594 - val_accuracy: 0.6327\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0012959999497979878.\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6722 - accuracy: 0.6074 - val_loss: 0.6181 - val_accuracy: 0.6735\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.61167\n",
      "Epoch 21/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.6197 - accuracy: 0.6516 - val_loss: 0.6171 - val_accuracy: 0.6837\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.61167\n",
      "Epoch 22/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6374 - accuracy: 0.6469 - val_loss: 0.6171 - val_accuracy: 0.6837\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.61167\n",
      "Epoch 23/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6032 - accuracy: 0.6575 - val_loss: 0.6212 - val_accuracy: 0.6633\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0007775999838486314.\n",
      "Epoch 24/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6177 - accuracy: 0.6779 - val_loss: 0.6161 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.61167\n",
      "Epoch 25/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.5879 - accuracy: 0.6907 - val_loss: 0.6191 - val_accuracy: 0.6633\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.61167\n",
      "Epoch 26/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6244 - accuracy: 0.6474 - val_loss: 0.6169 - val_accuracy: 0.6837\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.61167\n",
      "Epoch 27/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6350 - accuracy: 0.6272 - val_loss: 0.6226 - val_accuracy: 0.6633\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0004665599903091788.\n",
      "Epoch 28/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.6189 - accuracy: 0.6672 - val_loss: 0.6170 - val_accuracy: 0.6735\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.61167\n",
      "Epoch 29/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6133 - accuracy: 0.6587 - val_loss: 0.6161 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.61167\n",
      "Epoch 30/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6157 - accuracy: 0.6517 - val_loss: 0.6160 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.61167\n",
      "Epoch 31/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6259 - accuracy: 0.6522 - val_loss: 0.6167 - val_accuracy: 0.6837\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00027993599069304765.\n",
      "Epoch 32/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6161 - accuracy: 0.6388 - val_loss: 0.6187 - val_accuracy: 0.6735\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.61167\n",
      "Epoch 33/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6207 - accuracy: 0.6400 - val_loss: 0.6169 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.61167\n",
      "Epoch 34/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6202 - accuracy: 0.6320 - val_loss: 0.6196 - val_accuracy: 0.6633\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.61167\n",
      "Epoch 35/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6061 - accuracy: 0.6820 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00016796158743090928.\n",
      "Epoch 36/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6047 - accuracy: 0.6761 - val_loss: 0.6169 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.61167\n",
      "Epoch 37/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6221 - accuracy: 0.6353 - val_loss: 0.6172 - val_accuracy: 0.6837\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.61167\n",
      "Epoch 38/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6258 - accuracy: 0.6415 - val_loss: 0.6169 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.61167\n",
      "Epoch 39/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6133 - accuracy: 0.6589 - val_loss: 0.6171 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.00010077694896608591.\n",
      "Epoch 40/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6265 - accuracy: 0.6405 - val_loss: 0.6181 - val_accuracy: 0.6837\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.61167\n",
      "Epoch 41/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6132 - accuracy: 0.6649 - val_loss: 0.6171 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.61167\n",
      "Epoch 42/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.5969 - accuracy: 0.6931 - val_loss: 0.6173 - val_accuracy: 0.6837\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.61167\n",
      "Epoch 43/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6037 - accuracy: 0.6777 - val_loss: 0.6173 - val_accuracy: 0.6837\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 6.046616763342172e-05.\n",
      "Epoch 44/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6025 - accuracy: 0.6779 - val_loss: 0.6173 - val_accuracy: 0.6837\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.61167\n",
      "Epoch 45/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6081 - accuracy: 0.6580 - val_loss: 0.6174 - val_accuracy: 0.6837\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.61167\n",
      "Epoch 46/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6203 - accuracy: 0.6518 - val_loss: 0.6171 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.61167\n",
      "Epoch 47/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6134 - accuracy: 0.6484 - val_loss: 0.6169 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 3.6279701453167947e-05.\n",
      "Epoch 48/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6276 - accuracy: 0.6339 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.61167\n",
      "Epoch 49/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6235 - accuracy: 0.6401 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.61167\n",
      "Epoch 50/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6222 - accuracy: 0.6466 - val_loss: 0.6168 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.61167\n",
      "Epoch 51/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6094 - accuracy: 0.6419 - val_loss: 0.6170 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 2.1767819998785853e-05.\n",
      "Epoch 52/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6474 - accuracy: 0.6128 - val_loss: 0.6170 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.61167\n",
      "Epoch 53/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6131 - accuracy: 0.6418 - val_loss: 0.6169 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.61167\n",
      "Epoch 54/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6532 - accuracy: 0.6089 - val_loss: 0.6172 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.61167\n",
      "Epoch 55/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6139 - accuracy: 0.6586 - val_loss: 0.6170 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1.3060692435828968e-05.\n",
      "Epoch 56/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6171 - accuracy: 0.6537 - val_loss: 0.6170 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.61167\n",
      "Epoch 57/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6190 - accuracy: 0.6273 - val_loss: 0.6170 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.61167\n",
      "Epoch 58/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6375 - accuracy: 0.6396 - val_loss: 0.6171 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.61167\n",
      "Epoch 59/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.6197 - accuracy: 0.6532 - val_loss: 0.6170 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 7.836415352358016e-06.\n",
      "Epoch 60/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6221 - accuracy: 0.6458 - val_loss: 0.6170 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.61167\n",
      "Epoch 61/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6249 - accuracy: 0.6459 - val_loss: 0.6170 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.61167\n",
      "Epoch 62/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6209 - accuracy: 0.6479 - val_loss: 0.6171 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.61167\n",
      "Epoch 63/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6218 - accuracy: 0.6425 - val_loss: 0.6170 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 4.7018493205541745e-06.\n",
      "Epoch 64/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6188 - accuracy: 0.6501 - val_loss: 0.6170 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.61167\n",
      "Epoch 65/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6290 - accuracy: 0.6066 - val_loss: 0.6170 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.61167\n",
      "Epoch 66/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6242 - accuracy: 0.6503 - val_loss: 0.6170 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.61167\n",
      "Epoch 67/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6107 - accuracy: 0.6514 - val_loss: 0.6170 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 2.8211095923325045e-06.\n",
      "Epoch 68/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6210 - accuracy: 0.6619 - val_loss: 0.6170 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.61167\n",
      "Epoch 69/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6131 - accuracy: 0.6473 - val_loss: 0.6170 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.61167\n",
      "Epoch 70/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6263 - accuracy: 0.6344 - val_loss: 0.6170 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.61167\n",
      "Epoch 71/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6292 - accuracy: 0.6304 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 1.6926657281146617e-06.\n",
      "Epoch 72/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6202 - accuracy: 0.6272 - val_loss: 0.6171 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.61167\n",
      "Epoch 73/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6251 - accuracy: 0.6473 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.61167\n",
      "Epoch 74/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6109 - accuracy: 0.6652 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.61167\n",
      "Epoch 75/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5994 - accuracy: 0.6744 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 1.0155994232263765e-06.\n",
      "Epoch 76/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6126 - accuracy: 0.6491 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.61167\n",
      "Epoch 77/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6225 - accuracy: 0.6364 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.61167\n",
      "Epoch 78/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6110 - accuracy: 0.6501 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.61167\n",
      "Epoch 79/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6189 - accuracy: 0.6645 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 6.093596539358259e-07.\n",
      "Epoch 80/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6120 - accuracy: 0.6524 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.61167\n",
      "Epoch 81/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6003 - accuracy: 0.6818 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.61167\n",
      "Epoch 82/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6094 - accuracy: 0.6564 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.61167\n",
      "Epoch 83/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6360 - accuracy: 0.6521 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 3.65615778719075e-07.\n",
      "Epoch 84/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.5983 - accuracy: 0.6838 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.61167\n",
      "Epoch 85/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6484 - accuracy: 0.6231 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.61167\n",
      "Epoch 86/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6292 - accuracy: 0.6415 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.61167\n",
      "Epoch 87/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6246 - accuracy: 0.6468 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 2.1936947405265526e-07.\n",
      "Epoch 88/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6233 - accuracy: 0.6597 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.61167\n",
      "Epoch 89/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6332 - accuracy: 0.6312 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.61167\n",
      "Epoch 90/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.5926 - accuracy: 0.6821 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.61167\n",
      "Epoch 91/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5984 - accuracy: 0.6761 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 1.3162168102098804e-07.\n",
      "Epoch 92/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6208 - accuracy: 0.6356 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.61167\n",
      "Epoch 93/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6242 - accuracy: 0.6268 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.61167\n",
      "Epoch 94/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.5758 - accuracy: 0.7019 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.61167\n",
      "Epoch 95/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6168 - accuracy: 0.6450 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 7.897300520198769e-08.\n",
      "Epoch 96/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6149 - accuracy: 0.6545 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.61167\n",
      "Epoch 97/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6291 - accuracy: 0.6308 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.61167\n",
      "Epoch 98/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6184 - accuracy: 0.6577 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.61167\n",
      "Epoch 99/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.6217 - accuracy: 0.6270 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 4.738380141589005e-08.\n",
      "Epoch 100/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6064 - accuracy: 0.6779 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.61167\n",
      "Epoch 101/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6154 - accuracy: 0.6532 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.61167\n",
      "Epoch 102/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6211 - accuracy: 0.6439 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.61167\n",
      "Epoch 103/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6186 - accuracy: 0.6572 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 2.8430279996882744e-08.\n",
      "Epoch 104/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6201 - accuracy: 0.6549 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.61167\n",
      "Epoch 105/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6260 - accuracy: 0.6362 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.61167\n",
      "Epoch 106/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6067 - accuracy: 0.6656 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.61167\n",
      "Epoch 107/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6207 - accuracy: 0.6463 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 1.7058168211292468e-08.\n",
      "Epoch 108/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6088 - accuracy: 0.6638 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.61167\n",
      "Epoch 109/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6010 - accuracy: 0.6631 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.61167\n",
      "Epoch 110/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6235 - accuracy: 0.6415 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.61167\n",
      "Epoch 111/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6257 - accuracy: 0.6559 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 1.0234900926775481e-08.\n",
      "Epoch 112/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6181 - accuracy: 0.6601 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.61167\n",
      "Epoch 113/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6157 - accuracy: 0.6457 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.61167\n",
      "Epoch 114/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6141 - accuracy: 0.6488 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.61167\n",
      "Epoch 115/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6358 - accuracy: 0.6489 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.61167\n",
      "\n",
      "Epoch 00115: ReduceLROnPlateau reducing learning rate to 1e-08.\n",
      "Epoch 116/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.5993 - accuracy: 0.6819 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.61167\n",
      "Epoch 117/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6104 - accuracy: 0.6621 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.61167\n",
      "Epoch 118/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6002 - accuracy: 0.6939 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.61167\n",
      "Epoch 119/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6340 - accuracy: 0.6462 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.61167\n",
      "Epoch 120/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6345 - accuracy: 0.6201 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.61167\n",
      "Epoch 121/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6134 - accuracy: 0.6561 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.61167\n",
      "Epoch 122/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6183 - accuracy: 0.6521 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.61167\n",
      "Epoch 123/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6129 - accuracy: 0.6490 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.61167\n",
      "Epoch 124/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6198 - accuracy: 0.6339 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.61167\n",
      "Epoch 125/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.5965 - accuracy: 0.6847 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.61167\n",
      "Epoch 126/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6071 - accuracy: 0.6732 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.61167\n",
      "Epoch 127/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6183 - accuracy: 0.6407 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.61167\n",
      "Epoch 128/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6234 - accuracy: 0.6343 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.61167\n",
      "Epoch 129/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.5999 - accuracy: 0.6795 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.61167\n",
      "Epoch 130/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6249 - accuracy: 0.6315 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.61167\n",
      "Epoch 131/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6431 - accuracy: 0.6292 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.61167\n",
      "Epoch 132/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6310 - accuracy: 0.6328 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.61167\n",
      "Epoch 133/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6255 - accuracy: 0.6443 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.61167\n",
      "Epoch 134/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6273 - accuracy: 0.6380 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.61167\n",
      "Epoch 135/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6351 - accuracy: 0.6361 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.61167\n",
      "Epoch 136/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6175 - accuracy: 0.6536 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.61167\n",
      "Epoch 137/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6085 - accuracy: 0.6474 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.61167\n",
      "Epoch 138/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6290 - accuracy: 0.6504 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.61167\n",
      "Epoch 139/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6177 - accuracy: 0.6535 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.61167\n",
      "Epoch 140/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6183 - accuracy: 0.6575 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.61167\n",
      "Epoch 141/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6161 - accuracy: 0.6630 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.61167\n",
      "Epoch 142/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6012 - accuracy: 0.6618 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.61167\n",
      "Epoch 143/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6298 - accuracy: 0.6431 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.61167\n",
      "Epoch 144/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6150 - accuracy: 0.6459 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.61167\n",
      "Epoch 145/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6107 - accuracy: 0.6536 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.61167\n",
      "Epoch 146/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6262 - accuracy: 0.6346 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.61167\n",
      "Epoch 147/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6065 - accuracy: 0.6508 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.61167\n",
      "Epoch 148/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6098 - accuracy: 0.6418 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.61167\n",
      "Epoch 149/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6159 - accuracy: 0.6418 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.61167\n",
      "Epoch 150/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6161 - accuracy: 0.6601 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.61167\n",
      "Epoch 151/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6141 - accuracy: 0.6647 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.61167\n",
      "Epoch 152/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6254 - accuracy: 0.6349 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.61167\n",
      "Epoch 153/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6320 - accuracy: 0.6376 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.61167\n",
      "Epoch 154/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.5948 - accuracy: 0.6845 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.61167\n",
      "Epoch 155/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.5981 - accuracy: 0.6760 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.61167\n",
      "Epoch 156/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6223 - accuracy: 0.6463 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.61167\n",
      "Epoch 157/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6227 - accuracy: 0.6526 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.61167\n",
      "Epoch 158/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5924 - accuracy: 0.6694 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.61167\n",
      "Epoch 159/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6210 - accuracy: 0.6574 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.61167\n",
      "Epoch 160/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6011 - accuracy: 0.6757 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.61167\n",
      "Epoch 161/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6132 - accuracy: 0.6530 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.61167\n",
      "Epoch 162/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6284 - accuracy: 0.6389 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.61167\n",
      "Epoch 163/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6280 - accuracy: 0.6225 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.61167\n",
      "Epoch 164/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6013 - accuracy: 0.6820 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.61167\n",
      "Epoch 165/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6408 - accuracy: 0.6197 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.61167\n",
      "Epoch 166/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6152 - accuracy: 0.6588 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.61167\n",
      "Epoch 167/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6297 - accuracy: 0.6560 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.61167\n",
      "Epoch 168/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.5943 - accuracy: 0.6520 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.61167\n",
      "Epoch 169/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6170 - accuracy: 0.6490 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.61167\n",
      "Epoch 170/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6082 - accuracy: 0.6492 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.61167\n",
      "Epoch 171/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6293 - accuracy: 0.6402 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.61167\n",
      "Epoch 172/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6264 - accuracy: 0.6445 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.61167\n",
      "Epoch 173/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6000 - accuracy: 0.6716 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.61167\n",
      "Epoch 174/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6232 - accuracy: 0.6419 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.61167\n",
      "Epoch 175/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6088 - accuracy: 0.6656 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.61167\n",
      "Epoch 176/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6225 - accuracy: 0.6227 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.61167\n",
      "Epoch 177/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6410 - accuracy: 0.6331 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.61167\n",
      "Epoch 178/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6157 - accuracy: 0.6371 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.61167\n",
      "Epoch 179/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6178 - accuracy: 0.6549 - val_loss: 0.6171 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.61167\n",
      "Epoch 180/200\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6778 - accuracy: 0.6250"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32/219968277.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model.fit(x=TRAIN_x, y=TRAIN_y, epochs=200, shuffle=True,\n\u001b[0m\u001b[0;32m      2\u001b[0m           batch_size=32, callbacks=CALLBACK, validation_data = (VALID_x, VALID_y))\n",
      "\u001b[1;32mc:\\users\\sosal\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sosal\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sosal\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sosal\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sosal\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\users\\sosal\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sosal\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x=TRAIN_x, y=TRAIN_y, epochs=200, shuffle=True,\n",
    "          batch_size=32, callbacks=CALLBACK, validation_data = (VALID_x, VALID_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb1857b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('-011-0.6778-0.5918-0.6678.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "deb3d40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4453733 ],\n",
       "       [0.3992342 ],\n",
       "       [0.47151455],\n",
       "       [0.25959498],\n",
       "       [0.4810236 ],\n",
       "       [0.525337  ],\n",
       "       [0.3065454 ],\n",
       "       [0.5190637 ],\n",
       "       [0.55606884],\n",
       "       [0.5325421 ],\n",
       "       [0.58463776],\n",
       "       [0.4826668 ],\n",
       "       [0.57039464],\n",
       "       [0.5579886 ],\n",
       "       [0.70330054],\n",
       "       [0.50074613],\n",
       "       [0.2673866 ],\n",
       "       [0.36141443],\n",
       "       [0.24365804],\n",
       "       [0.2287034 ],\n",
       "       [0.18412009],\n",
       "       [0.21114911],\n",
       "       [0.30264673],\n",
       "       [0.2640684 ],\n",
       "       [0.398275  ],\n",
       "       [0.43198514],\n",
       "       [0.38023058],\n",
       "       [0.27718014],\n",
       "       [0.31826138],\n",
       "       [0.4539213 ],\n",
       "       [0.21778858],\n",
       "       [0.1914539 ],\n",
       "       [0.68346834],\n",
       "       [0.28908867],\n",
       "       [0.470838  ],\n",
       "       [0.6214945 ],\n",
       "       [0.2555138 ],\n",
       "       [0.21165326],\n",
       "       [0.23679332],\n",
       "       [0.646014  ],\n",
       "       [0.2703391 ],\n",
       "       [0.4352468 ],\n",
       "       [0.59612113],\n",
       "       [0.65621346],\n",
       "       [0.6246671 ],\n",
       "       [0.1884915 ],\n",
       "       [0.27159938],\n",
       "       [0.24365804],\n",
       "       [0.5269116 ],\n",
       "       [0.4776466 ],\n",
       "       [0.522952  ],\n",
       "       [0.32824996],\n",
       "       [0.401803  ],\n",
       "       [0.22183713],\n",
       "       [0.43419877],\n",
       "       [0.67870504],\n",
       "       [0.42838296],\n",
       "       [0.3901427 ],\n",
       "       [0.46650535],\n",
       "       [0.24070738],\n",
       "       [0.29165328],\n",
       "       [0.6425045 ],\n",
       "       [0.26382235],\n",
       "       [0.19254358],\n",
       "       [0.49540782],\n",
       "       [0.3619936 ],\n",
       "       [0.2395093 ],\n",
       "       [0.63908756],\n",
       "       [0.3525396 ],\n",
       "       [0.61552036],\n",
       "       [0.21446824],\n",
       "       [0.55894464],\n",
       "       [0.15414956],\n",
       "       [0.59121996],\n",
       "       [0.42427588],\n",
       "       [0.33171108],\n",
       "       [0.5857129 ],\n",
       "       [0.3509256 ],\n",
       "       [0.242924  ],\n",
       "       [0.5123118 ],\n",
       "       [0.2753117 ],\n",
       "       [0.28250298],\n",
       "       [0.6366112 ],\n",
       "       [0.5476578 ],\n",
       "       [0.30583915],\n",
       "       [0.19868302],\n",
       "       [0.4500557 ],\n",
       "       [0.36247048],\n",
       "       [0.49982142],\n",
       "       [0.3595545 ],\n",
       "       [0.5756336 ],\n",
       "       [0.5040729 ],\n",
       "       [0.34129295],\n",
       "       [0.3693161 ],\n",
       "       [0.5372507 ],\n",
       "       [0.55892426],\n",
       "       [0.19820888],\n",
       "       [0.26938167]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(VALID_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4437d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalPrediction = pd.DataFrame({\n",
    "    'Label': VALID_y['Relapse'],\n",
    "    'Prediction': model.predict(VALID_x)[:,0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99dab874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f53e4fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Label', ylabel='Prediction'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seaborn.boxplot(data=FinalPrediction, x='Label', y='Prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7092ba",
   "metadata": {},
   "source": [
    "# AUROC 및 ROC curve 그리기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d981e",
   "metadata": {},
   "source": [
    "## 당연히 Validation set에서 보는것이 맞음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5c71a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e01576a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6901315789473684\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(VALID_y, model.predict(VALID_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fa6c0ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# ROC curve 시각화\n",
    "Label = VALID_y\n",
    "pred = model.predict(VALID_x)\n",
    "fpr, tpr, _ = metrics.roc_curve(Label,  pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b841e6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "94007944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.696836203444133\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(roc_auc_score(TRAIN_y, model.predict(TRAIN_x)))\n",
    "# ROC curve 시각화\n",
    "Label = TRAIN_y\n",
    "pred = model.predict(TRAIN_x)\n",
    "fpr, tpr, _ = metrics.roc_curve(Label,  pred)\n",
    "\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
