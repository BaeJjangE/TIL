{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4021b4a",
   "metadata": {},
   "source": [
    "## 참고: https://hleecaster.com/ml-linear-regression-example/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d4f2b0",
   "metadata": {},
   "source": [
    "## 라이브러리 설치, 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2853ab4",
   "metadata": {},
   "source": [
    "!pip3 install -U scikit-learn<br>\n",
    "!pip3 install pandas<br>\n",
    "!pip3 install numpy<br>\n",
    "!pip3 install matplotlib<br>\n",
    "!pip3 install statsmodels<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c2d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a92498",
   "metadata": {},
   "source": [
    "## 데이터 다운로드 (특별할인 판매)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbdefba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date weekday  busy_day  high_temperature  special_sales\n",
      "0  2002-08-05     Mon         0                28              1\n",
      "1  2002-08-06     Tue         0                24              0\n",
      "2  2002-08-07     Wed         1                26              0\n",
      "3  2002-08-08     Thu         0                24              0\n",
      "4  2002-08-09     Fri         0                23              0\n",
      "(21, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/jmnote/zdata/master/logistic-regression/special-sales.csv')\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738d1868",
   "metadata": {},
   "source": [
    "## Input, Feature 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "095048a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = df['special_sales']\n",
    "InputFeature = df[['busy_day','high_temperature']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ec0b3",
   "metadata": {},
   "source": [
    "## Keras Logit 모델 fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fdfaee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(3, activation='linear', input_shape=(2,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daccfab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 3)                 9         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 13\n",
      "Trainable params: 13\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d903ad",
   "metadata": {},
   "source": [
    "## Keras 모델 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44239999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call-back 함수\n",
    "# CheckPoint: Epoch 마다 validation 성능을 검증하여, best performance 일 경우 저장\n",
    "CP = ModelCheckpoint(filepath='-{epoch:03d}-{loss:.4f}-{accuracy:.4f}.hdf5',\n",
    "            monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Learning Rate 줄여나가기\n",
    "# patience=3 loss값이 3번연속 떨어질시? factor=0.8 러닝레이트에 0.8을 곱해서 더 느리게 적용하게\n",
    "# min_lr = 1e-8 이하로는 내려가지말라.\n",
    "LR = ReduceLROnPlateau(monitor='loss',factor=0.8, patience=3, verbose=1, min_lr=1e-8)\n",
    "\n",
    "CALLBACK = [CP, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75ac0716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5537 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.55368, saving model to -001-0.5537-0.7619.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5786 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00002: loss did not improve from 0.55368\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5590 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00003: loss did not improve from 0.55368\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5440 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00004: loss improved from 0.55368 to 0.54395, saving model to -004-0.5440-0.7619.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00005: loss improved from 0.54395 to 0.53360, saving model to -005-0.5336-0.7619.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5769 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00006: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5588 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00007: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5558 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00008: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5518 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00009: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5519 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00010: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5465 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00011: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5602 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00012: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5610 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00013: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5819 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00014: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5482 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00015: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5770 - accuracy: 0.7143\n",
      "\n",
      "Epoch 00016: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5381 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00017: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6104 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00018: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5506 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00019: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5531 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00020: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5733 - accuracy: 0.6667\n",
      "\n",
      "Epoch 00021: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5797 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00022: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5483 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00023: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5678 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00024: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5385 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00025: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5515 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00026: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5550 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5500 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5418 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00029: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5679 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5391 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00031: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5583 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00032: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5488 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00033: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5483 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00034: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5399 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00035: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5637 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5990 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00037: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5396 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5682 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5812 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5463 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5476 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5523 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00043: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5603 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00044: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5451 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5621 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00046: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6075 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5483 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00048: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5428 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00049: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5442 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00050: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5504 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00051: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5598 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00052: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5417 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00053: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5482 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00054: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5389 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00055: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5398 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00056: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5545 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00057: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5418 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00058: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6251 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00059: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5426 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00060: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5450 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00061: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5425 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00062: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5371 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00063: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5380 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00064: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5678 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00065: loss did not improve from 0.53360\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5223 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00066: loss improved from 0.53360 to 0.52231, saving model to -066-0.5223-0.7619.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5486 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00067: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5408 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00068: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5301 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00069: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5527 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00070: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5374 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00071: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5367 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00072: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5401 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00073: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5815 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00074: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5307 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00075: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5359 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00076: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5608 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00077: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5734 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00078: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5248 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00079: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5356 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5789 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00081: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5583 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00082: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5447 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00083: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 0s/step - loss: 0.5246 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00084: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5515 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00085: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5268 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00086: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5292 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00087: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5365 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00088: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5493 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00089: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5662 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00090: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5392 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00091: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5420 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00092: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5388 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00093: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5490 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00094: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5587 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00095: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5376 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00096: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00097: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5309 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00098: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5303 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5302 - accuracy: 0.7619\n",
      "\n",
      "Epoch 00100: loss did not improve from 0.52231\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eb283c3bb0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=InputFeature, y=Label, epochs=100, shuffle=True, batch_size=3, callbacks=CALLBACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d0971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(...) # 저장된 모델 중 가장 베스트(가장 마지막에 저장된) 이름을 ...에 넣어준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb3d40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32733634],\n",
       "       [0.29245824],\n",
       "       [0.7862984 ],\n",
       "       [0.29245824],\n",
       "       [0.28408667],\n",
       "       [0.7996912 ],\n",
       "       [0.77226555],\n",
       "       [0.3096265 ],\n",
       "       [0.30097234],\n",
       "       [0.7996912 ],\n",
       "       [0.26778525],\n",
       "       [0.27586135],\n",
       "       [0.7930751 ],\n",
       "       [0.7862984 ],\n",
       "       [0.3096265 ],\n",
       "       [0.26778525],\n",
       "       [0.75002277],\n",
       "       [0.31841573],\n",
       "       [0.28408667],\n",
       "       [0.757595  ],\n",
       "       [0.7722655 ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(InputFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4437d070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.predict(InputFeature), df['busy_day'], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8a4f638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.5530\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5446\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5377\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5322\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5282\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5256\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5242\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5238\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5241\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5249\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5258\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5266\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5272\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5275\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5274\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5270\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5264\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5258\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5251\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5245\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5240\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5237\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5235\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5235\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5236\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5238\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5240\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5241\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5242\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5242\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5241\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5240\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5238\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5236\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5235\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5233\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5233\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5232\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5232\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5232\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5232\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5233\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5233\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5233\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5232\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5232\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5231\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5230\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5230\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5229\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5229\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5229\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5228\n",
      "Epoch 54/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5228\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5228\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5228\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5228\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5227\n",
      "Epoch 59/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5227\n",
      "Epoch 60/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5227\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5226\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5226\n",
      "Epoch 63/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5226\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5225\n",
      "Epoch 65/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5225\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5225\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5224\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5224\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5224\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5224\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5223\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5223\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5223\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5222\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5222\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5222\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5221\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5221\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5221\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5220\n",
      "Epoch 81/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5220\n",
      "Epoch 82/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5220\n",
      "Epoch 83/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5219\n",
      "Epoch 84/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5219\n",
      "Epoch 85/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5219\n",
      "Epoch 86/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5219\n",
      "Epoch 87/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5218\n",
      "Epoch 88/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5218\n",
      "Epoch 89/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5218\n",
      "Epoch 90/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5217\n",
      "Epoch 91/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5217\n",
      "Epoch 92/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5217\n",
      "Epoch 93/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5216\n",
      "Epoch 94/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5216\n",
      "Epoch 95/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5216\n",
      "Epoch 96/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5215\n",
      "Epoch 97/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5215\n",
      "Epoch 98/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5215\n",
      "Epoch 99/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5214\n",
      "Epoch 100/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5214\n",
      "Epoch 101/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5214\n",
      "Epoch 102/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5213\n",
      "Epoch 103/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5213\n",
      "Epoch 104/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5213\n",
      "Epoch 105/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5212\n",
      "Epoch 106/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5212\n",
      "Epoch 107/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5211\n",
      "Epoch 108/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5211\n",
      "Epoch 109/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5211\n",
      "Epoch 110/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5210\n",
      "Epoch 111/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5210\n",
      "Epoch 112/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5210\n",
      "Epoch 113/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5209\n",
      "Epoch 114/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5209\n",
      "Epoch 115/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5209\n",
      "Epoch 116/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5208\n",
      "Epoch 117/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5208\n",
      "Epoch 118/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5208\n",
      "Epoch 119/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5207\n",
      "Epoch 120/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5207\n",
      "Epoch 121/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5207\n",
      "Epoch 122/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5206\n",
      "Epoch 123/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5206\n",
      "Epoch 124/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5205\n",
      "Epoch 125/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5205\n",
      "Epoch 126/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5205\n",
      "Epoch 127/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5204\n",
      "Epoch 128/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5204\n",
      "Epoch 129/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5204\n",
      "Epoch 130/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5203\n",
      "Epoch 131/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5203\n",
      "Epoch 132/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5203\n",
      "Epoch 133/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5202\n",
      "Epoch 134/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5202\n",
      "Epoch 135/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5201\n",
      "Epoch 136/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5201\n",
      "Epoch 137/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5201\n",
      "Epoch 138/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5200\n",
      "Epoch 139/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5200\n",
      "Epoch 140/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5200\n",
      "Epoch 141/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5199\n",
      "Epoch 142/300\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5199\n",
      "Epoch 143/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5198\n",
      "Epoch 144/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5198\n",
      "Epoch 145/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5198\n",
      "Epoch 146/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5197\n",
      "Epoch 147/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5197\n",
      "Epoch 148/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5197\n",
      "Epoch 149/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5196\n",
      "Epoch 150/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5196\n",
      "Epoch 151/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5195\n",
      "Epoch 152/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5195\n",
      "Epoch 153/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5195\n",
      "Epoch 154/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5194\n",
      "Epoch 155/300\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5194\n",
      "Epoch 156/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5193\n",
      "Epoch 157/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5193\n",
      "Epoch 158/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5193\n",
      "Epoch 159/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5192\n",
      "Epoch 160/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5192\n",
      "Epoch 161/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5192\n",
      "Epoch 162/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5191\n",
      "Epoch 163/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5191\n",
      "Epoch 164/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5190\n",
      "Epoch 165/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5190\n",
      "Epoch 166/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5190\n",
      "Epoch 167/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5189\n",
      "Epoch 168/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5189\n",
      "Epoch 169/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5188\n",
      "Epoch 170/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5188\n",
      "Epoch 171/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5188\n",
      "Epoch 172/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5187\n",
      "Epoch 173/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5187\n",
      "Epoch 174/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5186\n",
      "Epoch 175/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5186\n",
      "Epoch 176/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5186\n",
      "Epoch 177/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5185\n",
      "Epoch 178/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5185\n",
      "Epoch 179/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5184\n",
      "Epoch 180/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5184\n",
      "Epoch 181/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5184\n",
      "Epoch 182/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5183\n",
      "Epoch 183/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5183\n",
      "Epoch 184/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5182\n",
      "Epoch 185/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5182\n",
      "Epoch 186/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5182\n",
      "Epoch 187/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5181\n",
      "Epoch 188/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5181\n",
      "Epoch 189/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5180\n",
      "Epoch 190/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5180\n",
      "Epoch 191/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5180\n",
      "Epoch 192/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5179\n",
      "Epoch 193/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5179\n",
      "Epoch 194/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5178\n",
      "Epoch 195/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5178\n",
      "Epoch 196/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5178\n",
      "Epoch 197/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5177\n",
      "Epoch 198/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5177\n",
      "Epoch 199/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5176\n",
      "Epoch 200/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5176\n",
      "Epoch 201/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5175\n",
      "Epoch 202/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5175\n",
      "Epoch 203/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5175\n",
      "Epoch 204/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5174\n",
      "Epoch 205/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5174\n",
      "Epoch 206/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5173\n",
      "Epoch 207/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5173\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5173\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5172\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5172\n",
      "Epoch 211/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5171\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5171\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5170\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5170\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5170\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5169\n",
      "Epoch 217/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5169\n",
      "Epoch 218/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5168\n",
      "Epoch 219/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5168\n",
      "Epoch 220/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5168\n",
      "Epoch 221/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5167\n",
      "Epoch 222/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5167\n",
      "Epoch 223/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5166\n",
      "Epoch 224/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5166\n",
      "Epoch 225/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5165\n",
      "Epoch 226/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5165\n",
      "Epoch 227/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5165\n",
      "Epoch 228/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5164\n",
      "Epoch 229/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5164\n",
      "Epoch 230/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5163\n",
      "Epoch 231/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5163\n",
      "Epoch 232/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5162\n",
      "Epoch 233/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5162\n",
      "Epoch 234/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5162\n",
      "Epoch 235/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5161\n",
      "Epoch 236/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5161\n",
      "Epoch 237/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5160\n",
      "Epoch 238/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5160\n",
      "Epoch 239/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5159\n",
      "Epoch 240/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5159\n",
      "Epoch 241/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5159\n",
      "Epoch 242/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5158\n",
      "Epoch 243/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5158\n",
      "Epoch 244/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5157\n",
      "Epoch 245/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5157\n",
      "Epoch 246/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5156\n",
      "Epoch 247/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5156\n",
      "Epoch 248/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5156\n",
      "Epoch 249/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5155\n",
      "Epoch 250/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5155\n",
      "Epoch 251/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5154\n",
      "Epoch 252/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5154\n",
      "Epoch 253/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5153\n",
      "Epoch 254/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5153\n",
      "Epoch 255/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5153\n",
      "Epoch 256/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5152\n",
      "Epoch 257/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5152\n",
      "Epoch 258/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5151\n",
      "Epoch 259/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5151\n",
      "Epoch 260/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5150\n",
      "Epoch 261/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5150\n",
      "Epoch 262/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5149\n",
      "Epoch 263/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5149\n",
      "Epoch 264/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5149\n",
      "Epoch 265/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5148\n",
      "Epoch 266/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5148\n",
      "Epoch 267/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5147\n",
      "Epoch 268/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5147\n",
      "Epoch 269/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5146\n",
      "Epoch 270/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5146\n",
      "Epoch 271/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5146\n",
      "Epoch 272/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5145\n",
      "Epoch 273/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5145\n",
      "Epoch 274/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5144\n",
      "Epoch 275/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5144\n",
      "Epoch 276/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5143\n",
      "Epoch 277/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5143\n",
      "Epoch 278/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5142\n",
      "Epoch 279/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5142\n",
      "Epoch 280/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5142\n",
      "Epoch 281/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5141\n",
      "Epoch 282/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5141\n",
      "Epoch 283/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5140\n",
      "Epoch 284/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5140\n",
      "Epoch 285/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5139\n",
      "Epoch 286/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5139\n",
      "Epoch 287/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5138\n",
      "Epoch 288/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5138\n",
      "Epoch 289/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5138\n",
      "Epoch 290/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5137\n",
      "Epoch 291/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5137\n",
      "Epoch 292/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5136\n",
      "Epoch 293/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5136\n",
      "Epoch 294/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5135\n",
      "Epoch 295/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5135\n",
      "Epoch 296/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5134\n",
      "Epoch 297/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5134\n",
      "Epoch 298/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5134\n",
      "Epoch 299/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5133\n",
      "Epoch 300/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2e22947c8b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer=Adam(learning_rate=0.001))\n",
    "model.fit(x=InputFeature, y=Label, epochs=300, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5b6dd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.predict(InputFeature), df['busy_day'], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c71a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e01576a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8798076923076923\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(Label, model.predict(InputFeature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa6c0ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# ROC curve 시각화\n",
    "Labels = Label\n",
    "pred = model.predict(InputFeature)\n",
    "fpr, tpr, _ = metrics.roc_curve(Label,  pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b841e6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
