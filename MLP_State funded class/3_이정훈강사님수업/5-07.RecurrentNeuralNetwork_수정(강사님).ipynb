{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "84891dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, TimeDistributed, LSTM, GRU, Bidirectional\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943eb5c0",
   "metadata": {},
   "source": [
    "# 데이터 샘플 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4c7be156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 데이터 1개 생성\n",
    "Sample_x = [ 1,   0,   0, 0, 0, 0] \n",
    "Sample_y = [ 1, 0.8, 0.6, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4bfa43e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [1, 2, 3],\n",
       "       [1, 2, 3]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3] * 3).reshape(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2991afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 똑같은 데이터 50개로 증폭하여 샘플 생성\n",
    "sample_size = 50\n",
    "\n",
    "Train_x = np.array(Sample_x*sample_size).reshape(sample_size, len(Sample_x), 1)\n",
    "Train_y = np.array(Sample_y*sample_size).reshape(sample_size, len(Sample_y), 1)\n",
    "# Dimension : 샘플의 수, Time domain의 수, X변수의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5904333a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 6, 1)\n",
      "(50, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 shape 꼭 확인\n",
    "print(Train_x.shape)\n",
    "print(Train_y.shape)\n",
    "\n",
    "# Sample, Time-steps, Data Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c2122128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Sample_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9e3ac551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구성\n",
    "model = Sequential()\n",
    "\n",
    "# RNN. 현재 RNN은 return_sequences (모든 x time-step에 대하여 output을 냄)\n",
    "model.add(SimpleRNN(1, activation='tanh', \n",
    "                    return_sequences = True,\n",
    "                    input_shape= ( len(Sample_x), 1,  )\n",
    "                   ))\n",
    "\n",
    "model.add(TimeDistributed(Dense(1, activation='linear')))\n",
    "model.compile(loss = \"mse\", optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2ee453d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_51 (SimpleRNN)    (None, 6, 1)              3         \n",
      "_________________________________________________________________\n",
      "time_distributed_25 (TimeDis (None, 6, 1)              2         \n",
      "=================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조 확인\n",
    "# Simple_rnn의 Parameter의 수가 3인 이유는 1. coef A, 2. Const, 3. RNN State_const\n",
    "# TimeDistributed ( Dense ) 의 output shape가 왜 (None, 6, 1) 일까?\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "54c836e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50/50 [==============================] - 1s 6ms/step - loss: 0.2198\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1851\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1588\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1332\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2e29345d100>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model fitting\n",
    "model.fit(Train_x, Train_y, epochs=5, batch_size=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "83a03785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002E29345BC10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.5325564 ],\n",
       "        [0.409338  ],\n",
       "        [0.31561193],\n",
       "        [0.2412745 ],\n",
       "        [0.18159515],\n",
       "        [0.13383628]]], dtype=float32)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 1개로 prediction\n",
    "# Dimension 헷갈리지 않게 체크\n",
    "model.predict(\n",
    "    [\n",
    "        [[1], [0], [0], [0], [0], [0]]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "04e4a0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002E29345BC10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.5325564 ],\n",
       "        [0.409338  ],\n",
       "        [0.31561193],\n",
       "        [0.2412745 ],\n",
       "        [0.18159515],\n",
       "        [0.13383628]],\n",
       "\n",
       "       [[0.5325564 ],\n",
       "        [0.5157076 ],\n",
       "        [0.39689296],\n",
       "        [0.3058514 ],\n",
       "        [0.2334531 ],\n",
       "        [0.17531697]]], dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\n",
    "    [\n",
    "        [\n",
    "            [1], [ 0 ], [0], [0], [0], [0]]\n",
    "        ,\n",
    "        [\n",
    "            [1], [0.3], [0], [0], [0], [0]\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf733b9a",
   "metadata": {},
   "source": [
    "# Return_sequence가 없는 RNN 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c5fcd0",
   "metadata": {},
   "source": [
    "(에러나는 이유?)\n",
    " `TimeDistributed` Layer should be passed an `input_shape ` with at least 3 dimensions, received: [None, 1]\n",
    " input_shape = 최소 3 dimension\n",
    " 기대하는 Input shape: [batch_size, Time_domain, 변수의 수]\n",
    "                       [32,         6,         ,  1]\n",
    "                       return_sequence = True\n",
    "                       RNN -> output이 input으로 6개의 T를 -> 6개의 output\n",
    "                       [None, 6, 1]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2668fbeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`TimeDistributed` Layer should be passed an `input_shape ` with at least 3 dimensions, received: [None, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3840/4243296657.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# RNN. 현재 RNN은 return_sequences (모든 x time-step에 대하여 output을 냄)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSimpleRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tanh'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSample_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"mse\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sosal\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sosal\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    221\u001b[0m       \u001b[1;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m       \u001b[1;31m# refresh its output.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m       \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sosal\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    949\u001b[0m     \u001b[1;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[0;32m    952\u001b[0m                                                 input_list)\n\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sosal\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1088\u001b[0m           layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[0;32m   1089\u001b[0m         \u001b[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1090\u001b[1;33m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[0;32m   1091\u001b[0m             inputs, input_masks, args, kwargs)\n\u001b[0;32m   1092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sosal\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sosal\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    860\u001b[0m           \u001b[1;31m# overridden).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m           \u001b[1;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sosal\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2708\u001b[0m         \u001b[1;31m# operations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2709\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2710\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint:disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2711\u001b[0m       \u001b[1;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2712\u001b[0m       \u001b[1;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sosal\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m       raise ValueError(\n\u001b[0m\u001b[0;32m    176\u001b[0m           \u001b[1;34m'`TimeDistributed` Layer should be passed an `input_shape ` '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m           'with at least 3 dimensions, received: ' + str(input_shape))\n",
      "\u001b[1;31mValueError\u001b[0m: `TimeDistributed` Layer should be passed an `input_shape ` with at least 3 dimensions, received: [None, 1]"
     ]
    }
   ],
   "source": [
    "# 모델 구성\n",
    "model = Sequential()\n",
    "\n",
    "# RNN. 현재 RNN은 return_sequences (모든 x time-step에 대하여 output을 냄)\n",
    "model.add(SimpleRNN(1, activation='tanh', return_sequences = False, input_shape=( len(Sample_x), 1,  )))\n",
    "model.add(TimeDistributed(Dense(1, activation='linear')))\n",
    "model.compile(loss = \"mse\", optimizer='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ec03b",
   "metadata": {},
   "source": [
    "<h3>왜 에러가 나는지에 대한 이해</h3>\n",
    "<br>\n",
    "1. return_sequences를 False를 하면, RNN의 최종 Output만 출력<br>\n",
    "2. TimeDistributed는 이전의 Output sequence가 time-series로 출력될 것을 가정. 따라서 3차원의 output이 나올 것으로 예상<br>\n",
    "( batch_size (1차원) * 이전 Layer의 output (1차원) * Time-series (1차원) )<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ea321694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 데이터 1개 생성\n",
    "Sample_x = [1, 0, 0, 0, 0, 0]\n",
    "Sample_y = [0]\n",
    "\n",
    "# many to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "885293e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 똑같은 데이터 100개로 증폭하여 샘플 생성\n",
    "sample_size = 100\n",
    "\n",
    "Train_x = np.array(Sample_x*sample_size).reshape(sample_size, len(Sample_x), 1)\n",
    "Train_y = np.array(Sample_y*sample_size).reshape(sample_size, len(Sample_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "003dcc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 6, 1)\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(Train_x.shape)\n",
    "print(Train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a3e6a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구성\n",
    "model = Sequential()\n",
    "\n",
    "# RNN. 현재 RNN은 return_sequences (모든 x time-step에 대하여 output을 냄)\n",
    "model.add(SimpleRNN(1, activation='tanh', return_sequences = False, input_shape=( len(Sample_x), 1,  )))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "86fc78f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_55 (SimpleRNN)    (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "31ae4a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.4923\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3912\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3165\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.2615\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.2198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2e5a4b3d610>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model fitting\n",
    "model.fit(Train_x, Train_y, epochs=5, batch_size=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb7bce1",
   "metadata": {},
   "source": [
    "# Test 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f57a6a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002E5A4A37E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.17692749]], dtype=float32)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 1개로 prediction\n",
    "# Dimension 헷갈리지 않게 체크, output은 1개\n",
    "model.predict(\n",
    "    [\n",
    "        [[1], [0], [0], [0], [0], [0]]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2762f255",
   "metadata": {},
   "source": [
    "# 모델 구성도 상상해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c2e8752b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_40 (SimpleRNN)    (None, 6, 1)              3         \n",
      "_________________________________________________________________\n",
      "simple_rnn_41 (SimpleRNN)    (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 8\n",
      "Trainable params: 8\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구성\n",
    "model = Sequential()\n",
    "\n",
    "# RNN. 현재 RNN은 return_sequences (모든 x time-step에 대하여 output을 냄)\n",
    "model.add(SimpleRNN(1, activation='tanh', return_sequences = True, input_shape=( len(Sample_x), 1,  )))\n",
    "model.add(SimpleRNN(1, activation='tanh', return_sequences = False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer='Adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d7f295f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_42 (SimpleRNN)    (None, 6, 1)              3         \n",
      "_________________________________________________________________\n",
      "simple_rnn_43 (SimpleRNN)    (None, 6, 1)              3         \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 6, 1)              2         \n",
      "=================================================================\n",
      "Total params: 8\n",
      "Trainable params: 8\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구성\n",
    "model = Sequential()\n",
    "\n",
    "# RNN. 현재 RNN은 return_sequences (모든 x time-step에 대하여 output을 냄)\n",
    "model.add(SimpleRNN(1, activation='tanh', return_sequences = True, input_shape=( len(Sample_x), 1,  )))\n",
    "model.add(SimpleRNN(1, activation='tanh', return_sequences = True))\n",
    "model.add(TimeDistributed(Dense(1, activation='linear')))\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer='Adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "27d18060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_44 (SimpleRNN)    (None, 6, 1)              3         \n",
      "_________________________________________________________________\n",
      "simple_rnn_45 (SimpleRNN)    (None, 6, 1)              3         \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, 6, 1)              2         \n",
      "=================================================================\n",
      "Total params: 8\n",
      "Trainable params: 8\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구성\n",
    "model = Sequential()\n",
    "\n",
    "# RNN. 현재 RNN은 return_sequences (모든 x time-step에 대하여 output을 냄)\n",
    "model.add(SimpleRNN(1, activation='tanh', return_sequences = True, input_shape=( len(Sample_x), 1,  )))\n",
    "model.add(SimpleRNN(1, activation='tanh', return_sequences = True))\n",
    "model.add(TimeDistributed(Dense(1, activation='linear')))\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer='Adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c84c1f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_46 (SimpleRNN)    (None, 6, 5)              35        \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 6, 10)             360       \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 6, 10)             640       \n",
      "_________________________________________________________________\n",
      "time_distributed_22 (TimeDis (None, 6, 1)              11        \n",
      "=================================================================\n",
      "Total params: 1,046\n",
      "Trainable params: 1,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(5, return_sequences=True, input_shape=( len(Sample_x), 1,  )))\n",
    "model.add(Bidirectional(GRU(5, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(5, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(1, activation=\"relu\")))\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer='Adam')\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
